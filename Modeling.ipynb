{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data100k():\n",
    "    global PERC\n",
    "    df = read_process(\"./data/ml100k/u.data\", sep=\"\\t\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * PERC)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ML 100k dataset ###########\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "PERC=0.9\n",
    "USER_NUM = 943\n",
    "ITEM_NUM = 1682\n",
    "df_train, df_test = get_data100k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ML 100k dataset ###########\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "PERC=0.9\n",
    "USER_NUM = 943\n",
    "ITEM_NUM = 1682\n",
    "df_train, df_test = get_data100k()\n",
    "\n",
    "#Without Graph Feature\n",
    "MFSIZE=40\n",
    "UW=0.08\n",
    "IW=0.06\n",
    "LR=0.0002\n",
    "EPOCH_MAX = 601\n",
    "tf.reset_default_graph()\n",
    "GraphRec(df_train, df_test,ItemData=False,UserData=False,Graph=False,Dataset='100k')\n",
    "\n",
    "#With Graph Features\n",
    "MFSIZE=50\n",
    "UW=0.05\n",
    "IW=0.02\n",
    "LR=0.00003\n",
    "EPOCH_MAX = 196\n",
    "tf.reset_default_graph()\n",
    "GraphRec(df_train, df_test,ItemData=False,UserData=False,Graph=True,Dataset='100k')\n",
    "\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
    "        out = self.inputs[ids, :] #뭐임?\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "    \n",
    "\n",
    "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
    "    with tf.device(DEVICE): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\")\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\")\n",
    "        \n",
    "        \n",
    "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
    "\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\")\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\")\n",
    "\n",
    "    return infer, regularizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + dot.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
    "    with tf.device(DEVICE): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_iser,idx_item embedding\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\")\n",
    "        \n",
    "        \n",
    "        ul1mf=tf.layers.dense(idnputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf) #PQ 내적\n",
    "\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
    "\n",
    "    return infer, regularizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec(train, test,ItemData=False,UserData=False,Graph=False,Dataset='100k'):\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "    \n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in train.iterrows():\n",
    "      userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "      itemid=int(row['item'])\n",
    "      AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "      AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
    "      DegreeUsers[userid][0]+=1\n",
    "      DegreeItems[itemid][0]+=1\n",
    "    \n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "    \n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "    \n",
    "    if(Graph):\n",
    "        UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "        print(UserFeatures.shape) #\n",
    "        ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "    else:\n",
    "        UserFeatures=np.identity(USER_NUM,dtype=np.bool_)\n",
    "        ItemFeatures=np.identity(ITEM_NUM,dtype=np.bool_)\n",
    "\n",
    "\n",
    "\n",
    "#     if(UserData):\n",
    "#       if(Dataset=='1m'):\n",
    "#         UsrDat=get_UserData1M()\n",
    "#       if(Dataset=='100k'):\n",
    "#         UsrDat=get_UserData100k()\n",
    "#       UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "#     if(ItemData):\n",
    "#       if(Dataset=='1m'):\n",
    "#         ItmDat=get_ItemData1M()\n",
    "#       if(Dataset=='100k'):\n",
    "#         ItmDat=get_ItemData100k()\n",
    "\n",
    "#       ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "#     UserFeaturesLength=UserFeatures.shape[1]\n",
    "#     ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "#     print(UserFeatures.shape)\n",
    "#     print(ItemFeatures.shape)\n",
    "\n",
    "    \n",
    "#     samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "#     iter_train = ShuffleIterator([train[\"user\"],train[\"item\"],train[\"rate\"]],batch_size=BATCH_SIZE)\n",
    "\n",
    "#     iter_test = OneEpochIterator([test[\"user\"],test[\"item\"],test[\"rate\"]],batch_size=10000)\n",
    "\n",
    "\n",
    "#     user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "#     item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "#     rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "#     phase = tf.placeholder(tf.bool, name='phase')\n",
    "    \n",
    "    \n",
    "#     w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64)\n",
    "#     w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)\n",
    "\n",
    "\n",
    "#     infer, regularizer = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "#     global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "#     _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "#     finalerror=-1\n",
    "#     with tf.Session(config=config) as sess:\n",
    "#         sess.run(init_op)\n",
    "#         print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "#         errors = deque(maxlen=samples_per_batch)\n",
    "#         start = time.time()\n",
    "#         for i in range(EPOCH_MAX * samples_per_batch):\n",
    "#             #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "#             users, items, rates = next(iter_train)\n",
    "#             _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "#                                                                    item_batch: items,\n",
    "#                                                                    rate_batch: rates,\n",
    "#                                                                    phase:True})\n",
    "#             pred_batch = clip(pred_batch)\n",
    "#             errors.append(np.power(pred_batch - rates, 2))\n",
    "#             if i % samples_per_batch == 0:\n",
    "#                 train_err = np.sqrt(np.mean(errors))\n",
    "#                 test_err2 = np.array([])\n",
    "#                 degreelist=list()\n",
    "#                 predlist=list()\n",
    "#                 for users, items, rates in iter_test:\n",
    "#                     pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "#                                                             item_batch: items,                                                                                             \n",
    "#                                                             phase:False})\n",
    "\n",
    "#                     pred_batch = clip(pred_batch)            \n",
    "#                     test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "#                 end = time.time()\n",
    "#                 test_err = np.sqrt(np.mean(test_err2))\n",
    "#                 finalerror=test_err\n",
    "#                 print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "#                 start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 2626)\n"
     ]
    }
   ],
   "source": [
    " GraphRec(df_train,df_test,ItemData=True,UserData=True,Graph=True,Dataset='100k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
