{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from chart_studio.plotly import plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data100k():\n",
    "    global PERC\n",
    "    df = read_process(\"./data/ml100k/u.data\", sep=\"\\t\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * PERC)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "def get_UserData100k():\n",
    "    col_names = [\"user\", \"age\", \"gender\", \"occupation\", \"PostCode\"]\n",
    "    df = pd.read_csv('./data/ml100k/u.user', sep='|', header=None, names=col_names, engine='python')\n",
    "    del df[\"PostCode\"]\n",
    "    df[\"user\"] -= 1\n",
    "    df = pd.get_dummies(df, columns=[\"age\", \"gender\", \"occupation\"])\n",
    "    del df[\"user\"]\n",
    "    return df.values\n",
    "\n",
    "def get_ItemData100k():\n",
    "    col_names = [\"movieid\", \"movietitle\", \"releasedate\", \"videoreleasedate\", \"IMDbURL\"\n",
    "        , \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Childrens\", \"Comedy\", \"Crime\", \"Documentary\"\n",
    "        , \"Drama\", \"Fantasy\", \"FilmNoir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"SciFi\", \"Thriller\"\n",
    "        , \"War\", \"Western\"]\n",
    "    df = pd.read_csv('./data/ml100k/u.item', sep='|', header=None, names=col_names, engine='python')\n",
    "    df['releasedate'] = pd.to_datetime(df['releasedate'])\n",
    "    df['year'], df['month'] = zip(*df['releasedate'].map(lambda x: [x.year, x.month]))\n",
    "    df['year'] -= df['year'].min()\n",
    "    df['year'] /= df['year'].max()\n",
    "    df['year'] = df['year'].fillna(0.0)\n",
    "\n",
    "    del df[\"month\"]\n",
    "    del df[\"movietitle\"]\n",
    "    del df[\"releasedate\"]\n",
    "    del df[\"videoreleasedate\"]\n",
    "    del df[\"IMDbURL\"]\n",
    "\n",
    "    df[\"movieid\"] -= 1\n",
    "    del df[\"movieid\"]\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ML 100k dataset ###########\n",
    "DEVICE = \"/gpu:0\"\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "PERC=0.9\n",
    "USER_NUM = 943\n",
    "ITEM_NUM = 1682\n",
    "df_train, df_test = get_data100k()\n",
    "\n",
    "MFSIZE = 50\n",
    "UW = 0.05\n",
    "IW = 0.02\n",
    "LR = 0.00003\n",
    "EPOCH_MAX = 196\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
    "        out = self.inputs[ids, :] #뭐임?\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + dot.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1):\n",
    "    with tf.device(DEVICE): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_iser,idx_item embedding\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\")\n",
    "        \n",
    "        \n",
    "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf) #PQ 내적\n",
    "\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
    "\n",
    "    return infer, regularizer\n",
    "\n",
    "## Optimization\n",
    "\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
    "    with tf.device(DEVICE):\n",
    "        global_step = tf.train.get_global_step() #checkpoint\n",
    "        assert global_step is not None\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        cost = tf.add(cost_l2, regularizer)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뜯어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user_num = 943\n",
    "\n",
    "Item_num = 1682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 2710)\n",
      "(1682, 2646)\n"
     ]
    }
   ],
   "source": [
    "train,test = get_data100k() #train ,test 불러오기\n",
    "\n",
    "AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM),dtype = np.float32) # N X M의 zeros 행렬\n",
    "DegreeUsers = np.zeros((USER_NUM,1), dtype = np.float32) # N X 1\n",
    "AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM),dtype = np.float32)\n",
    "DegreeItems = np.zeros((ITEM_NUM,1), dtype = np.float32) # M X 1\n",
    "\n",
    "for index,row in train.iterrows():\n",
    "    userid = int(row['user']) #row는 row전체를 받으니 index마다의 user,item index를 저장\n",
    "    itemid = int(row['item'])\n",
    "    AdjacencyUsers[userid][itemid] = row['rate'] / 5.0 #저장한값에 rate부여\n",
    "    AdjacencyItems[itemid][userid] = row['rate'] / 5.0\n",
    "    DegreeUsers[userid][0] += 1 #user의 평가 횟수 +\n",
    "    DegreeItems[itemid][0] += 1 #item의 평가당하는 횟수 + \n",
    "\n",
    "DUserMax = np.amax(DegreeUsers) #array의 max값\n",
    "DItemMax = np.amax(DegreeItems)\n",
    "DegreeUsers = np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "DegreeItems = np.true_divide(DegreeItems, DItemMax)\n",
    "\n",
    "AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "\n",
    "#Graph = True 가정\n",
    "#np.identity(User_NUM) NXN 의 정방행렬\n",
    "# N X ( N + M + 1)  , M X ( N + M + 1) 행렬 생성\n",
    "UserFeatures = np.concatenate((np.identity(USER_NUM,dtype = np.bool_),AdjacencyUsers,DegreeUsers),axis = 1)\n",
    "ItemFeatures = np.concatenate((np.identity(ITEM_NUM,dtype = np.bool_),AdjacencyItems,DegreeItems),axis = 1)\n",
    "\n",
    "UsrDat = get_UserData100k() #943 X 84 , userfeatures 2D matrix, age + occupation + gender\n",
    "ItmDat = get_ItemData100k() #1682 X 20 , itemfeatures 2D matrix , 장르 19 , year 1개\n",
    "\n",
    "UserFeatures = np.concatenate((UserFeatures, UsrDat),axis =1) #N X (N + M + 1 + L)\n",
    "ItemFeatures = np.concatenate((ItemFeatures, ItmDat),axis =1) #M X (M + N + 1 + J)\n",
    "\n",
    "UserFeatureslength = UserFeatures.shape[1] #N + M + 1 + L\n",
    "ItemFeatureslength = ItemFeatures.shape[1] #M + N + 1 + J\n",
    "\n",
    "print(UserFeatures.shape)\n",
    "print(ItemFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000014F8308AEC8>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "samples_per_batch = len(train) // BATCH_SIZE #batch_size = 1000\n",
    "\n",
    "iter_train = ShuffleIterator([train['user'], train['item'],train['rate']], batch_size = BATCH_SIZE)\n",
    "iter_test = OneEpochIterator([test[\"user\"], test[\"item\"], test[\"rate\"]], batch_size=10000)\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape = [None], name = 'id_user')\n",
    "item_batch = tf.placeholder(tf.int32, shape = [None], name = 'id_item')\n",
    "rate_batch = tf.placeholder(tf.float64, shape = [None])\n",
    "phase = tf.placeholder(tf.bool, name = 'phase')\n",
    "\n",
    "w_user = tf.constant(UserFeatures, name=\"userids\", shape=[USER_NUM, UserFeatures.shape[1]], dtype=tf.float64)\n",
    "w_item = tf.constant(ItemFeatures, name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]], dtype=tf.float64)\n",
    "\n",
    "infer,regularizer = inferenceDense(phase, user_batch, item_batch, w_user, w_item, user_num=USER_NUM,item_num=ITEM_NUM) \n",
    "#return infer, regularizer\n",
    " \n",
    "global_step = tf.contrib.framework.get_or_create_global_step() # ?\n",
    "_, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09) #return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1d53878c8b0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}{}{}{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"train_error\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"val_error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "finalerror = -1\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"{}{}{}{}\".format(\"epoch\",\"train_error\",\"val_error\",\"elapsed_time\"))\n",
    "    errors = deque(maxlen = samples_per_batch) #batch사이즈를 최대길이로..?\n",
    "    start = time.time() #time check\n",
    "    for i in range(EPOCH_MAX * samples_per_batch):\n",
    "        #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "        users,items,rates = next(iter_train)\n",
    "        _, pred_batch = sess.run([train_op,infer], feed_dict = {user_batch : users,\n",
    "                                                                item_batch : items,\n",
    "                                                                rate_batch : rates,\n",
    "                                                                phase : True})\n",
    "        \n",
    "        pred_batch = clip(pred_batch) #pred_batch 1~5 사이\n",
    "        errors.append(np.power(pred_batch - rates , 2))\n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            degreelist = list()\n",
    "            predlist = list()\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict = {user_batch:users,\n",
    "                                                          item_batch:items,\n",
    "                                                          phase : False})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "            end = time.time()\n",
    "            test_err = np.sqrt(np.mean(test_err2))\n",
    "            finalerror = test_err\n",
    "            print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "            start = end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec(train, test,ItemData=False,UserData=False,Graph=False,Dataset='100k'):\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "    \n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in train.iterrows():\n",
    "      userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "      itemid=int(row['item'])\n",
    "      AdjacencyUsers[userid][itemid]=row['rate']/5.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "      AdjacencyItems[itemid][userid]=row['rate']/5.0 #동일, transpose matrix에\n",
    "      DegreeUsers[userid][0]+=1\n",
    "      DegreeItems[itemid][0]+=1\n",
    "    \n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "    \n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "    \n",
    "    if(Graph):\n",
    "        UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "        print(UserFeatures.shape) #\n",
    "        ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "    else:\n",
    "        UserFeatures=np.identity(USER_NUM,dtype=np.bool_)\n",
    "        ItemFeatures=np.identity(ITEM_NUM,dtype=np.bool_)\n",
    "\n",
    "\n",
    "\n",
    "    if(UserData):\n",
    "      if(Dataset=='1m'):\n",
    "        UsrDat=get_UserData1M()\n",
    "      if(Dataset=='100k'):\n",
    "        UsrDat=get_UserData100k()\n",
    "      UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "    if(ItemData):\n",
    "      if(Dataset=='1m'):\n",
    "        ItmDat=get_ItemData1M()\n",
    "      if(Dataset=='100k'):\n",
    "        ItmDat=get_ItemData100k()\n",
    "\n",
    "      ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "    UserFeaturesLength=UserFeatures.shape[1]\n",
    "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "    print(UserFeatures.shape)\n",
    "    print(ItemFeatures.shape)\n",
    "\n",
    "    \n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "    iter_train = ShuffleIterator([train[\"user\"],train[\"item\"],train[\"rate\"]],batch_size=BATCH_SIZE)\n",
    "\n",
    "    iter_test = OneEpochIterator([test[\"user\"],test[\"item\"],test[\"rate\"]],batch_size=10000)\n",
    "\n",
    "\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "    \n",
    "    \n",
    "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64)\n",
    "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)\n",
    "\n",
    "\n",
    "    infer, regularizer = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    finalerror=-1\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates,\n",
    "                                                                   phase:True})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                degreelist=list()\n",
    "                predlist=list()\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items,                                                                                             \n",
    "                                                            phase:False})\n",
    "\n",
    "                    pred_batch = clip(pred_batch)            \n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                finalerror=test_err\n",
    "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GraphRec(df_train,df_test,ItemData=True,UserData=True,Graph=True,Dataset='100k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
