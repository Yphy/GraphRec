{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터부르기 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_rt 생성 완료\n",
      "숫자바꾸기 완료\n",
      "split 완료\n"
     ]
    }
   ],
   "source": [
    "def make_userANDrate():\n",
    "    credentials = \"postgresql://jczdtzmaouemml:f3f55cc0c6bd25a42866864c9299f4ec79ff4ff890f6f69467e2b14dc0010074@ec2-3-215-207-12.compute-1.amazonaws.com:5432/de8i6u9p9i6vq8\"\n",
    "    dbdf = pd.read_sql(\"\"\"select distinct * from wearly_user where age <= 70 order by idx \"\"\", con = credentials)\n",
    "    item = pd.read_sql(\"\"\"select * from wearly_wear\"\"\", con = credentials)\n",
    "    df = dbdf.copy()\n",
    "    df = df.drop_duplicates(['name', 'gender', 'age']+ df.columns.tolist()[4:-1], keep='first').reset_index(drop=False)\n",
    "    df = df.drop(['index','name','idx','time'], axis=1 , inplace=False)\n",
    "    \n",
    "    print('데이터부르기 완료')\n",
    "\n",
    "    v = [data_row.values.tolist()[2:] for index, data_row in df.iterrows()]\n",
    "    vv = [v[i][j] for i in range(len(v)) for j in range(len(v[i]))]\n",
    "\n",
    "    user_rt = pd.DataFrame(index=range(0,len(df)*100) , columns=['user','image_file_name', 'rate'])\n",
    "    user_rt['user'] = sorted([i for i in range(0,len(df)) for j in range(0,100)])\n",
    "    user_rt['image_file_name'] = vv\n",
    "\n",
    "    for i in range(len(user_rt)):\n",
    "      user_rt['rate'][i] = int(user_rt['image_file_name'][i][-1])\n",
    "      user_rt['image_file_name'][i] = str(user_rt['image_file_name'][i][:-1])\n",
    "    \n",
    "    user_rt = user_rt.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    user_rt = user_rt.merge(item[['image_id', 'image_file_name']], on='image_file_name')\n",
    "    print('user_rt 생성 완료')\n",
    "    \n",
    "    user_rt.loc[user_rt['rate'] == 3 , 'rate'] = 2\n",
    "    user_rt.loc[user_rt['rate'] == 5 , 'rate'] = 3 \n",
    "    print('숫자바꾸기 완료')\n",
    "    PERC = 0.9\n",
    "    rows = len(user_rt)\n",
    "    split_index = int(rows * PERC)\n",
    "    df_train = user_rt[0:split_index]\n",
    "    df_test = user_rt[split_index:].reset_index(drop=True)\n",
    "    print('split 완료')\n",
    "    user_mt = df[['age','gender']]\n",
    "    user_mt = user_mt.reset_index()\n",
    "    user_mt['user'] = user_mt['index']\n",
    "    user_mt = user_mt[['user', 'age', 'gender']]\n",
    "    \n",
    "    # get one-hot encoding (gender)\n",
    "    user_mt = pd.get_dummies(user_mt, columns=[ \"age\", \"gender\"])\n",
    "  \n",
    "    return user_mt, df_train, df_test , item, user_rt\n",
    "\n",
    "usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-23-2e814775c8b9>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-2e814775c8b9>\"\u001b[1;36m, line \u001b[1;32m41\u001b[0m\n\u001b[1;33m    def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, MFSIZE=50, item_num,UReg=0.05,IReg=0.1, UW=0.05, IW=0.02):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "class ShuffleIterator(object):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
    "        out = self.inputs[ids, :] #뭐임?\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, MFSIZE=50, item_num,UReg=0.05,IReg=0.1, UW=0.05, IW=0.02):\n",
    "    with tf.device('/cpu:0'): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
    "                \n",
    "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(ul1mf.shape)\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(il1mf.shape)\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
    "        print(InferInputMF.shape)\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
    "    return infer, regularizer, ul1mf, il1mf\n",
    "\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
    "    with tf.device('/cpu:0'):\n",
    "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
    "        assert global_step is not None\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
    "        cost = tf.add(cost_l2, regularizer)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op\n",
    "\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 3.0) #벗어나는 값들 위치시키기\n",
    "\n",
    "def prediction_matrix(user_dict, item_dict):\n",
    "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
    "  for i in range(len(user_dict)):\n",
    "    for j in range(len(item_dict)):\n",
    "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
    "  return pred\n",
    "\n",
    "\n",
    "def recommender_for_user(users_items_matrix_df ,user_id, interact_matrix, df_content, topn = 6):\n",
    "    '''\n",
    "    Recommender Games for UserWarning\n",
    "    '''\n",
    "    pred_scores = interact_matrix.loc[user_id].values\n",
    "\n",
    "    df_scores   = pd.DataFrame({'image_id': list(users_items_matrix_df.columns), \n",
    "                               'score': pred_scores})\n",
    "\n",
    "    df_rec      = df_scores.set_index('image_id')\\\n",
    "                    .join(df_content.set_index('image_id'))\\\n",
    "                    .sort_values('score', ascending=False)\\\n",
    "                    .head(topn)[['score', 'image_file_name', 'hashtag_crawl']]\n",
    "    \n",
    "    return df_rec[df_rec.score > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RMSE(df1,df2):\n",
    "#     raw = df1\n",
    "    raw2 = df1.values.astype(np.float64)\n",
    "    pred = df2\n",
    "    \n",
    "    raw2[raw2 ==0] = 'nan'\n",
    "    \n",
    "    return np.sqrt(np.nansum((raw2 - pred)**2 / np.isfinite(raw2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9598431632269921"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_RMSE(users_items_matrix_df,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec(Epoch):\n",
    "    tf.reset_default_graph()\n",
    "    # usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
    "\n",
    "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
    "    BATCH_SIZE = 1000\n",
    "    DEVICE=\"/cpu:0\"\n",
    "    #With Graph Features\n",
    "    MFSIZE = 50\n",
    "    UW=0.05\n",
    "    IW=0.02\n",
    "    LR=0.00003\n",
    "    EPOCH_MAX = Epoch\n",
    "\n",
    "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
    "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name','comment_num','like_num'],axis=1, inplace=False)\n",
    "\n",
    "    UsrDat = UsrDat.values\n",
    "    ItmDat = ItmDat.values\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "\n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in df_train.iterrows():\n",
    "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "        itemid=int(row['image_id'])\n",
    "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
    "        DegreeUsers[userid][0]+=1\n",
    "        DegreeItems[itemid][0]+=1\n",
    "\n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "\n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "\n",
    "\n",
    "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "    print(UserFeatures.shape) #\n",
    "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "    UserFeaturesLength=UserFeatures.shape[1]\n",
    "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "    print(UserFeatures.shape)\n",
    "    print(ItemFeatures.shape)\n",
    "\n",
    "\n",
    "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
    "\n",
    "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
    "\n",
    "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
    "\n",
    "    #tensor 값을 할당할 placeholder 생성\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "    #tensor matrix생성\n",
    "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
    "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
    "\n",
    "\n",
    "    infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    finalerror=-1\n",
    "    #train_ls = []\n",
    "    #test_ls= []\n",
    "    p_ls = []\n",
    "    s_ls = []\n",
    "\n",
    "    p_dict = dict()\n",
    "    s_dict = dict()\n",
    "\n",
    "    total_df = user_rt.copy()\n",
    "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
    "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
    "                                                                  item_batch: items,\n",
    "                                                                  rate_batch: rates,\n",
    "                                                                  phase:True})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            #train_ls.append(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                degreelist=list()\n",
    "                predlist=list()\n",
    "                for users, items, rates in iter_test: #test의 pred_batch\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items,                                                                                             \n",
    "                                                            phase:False})\n",
    "\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    #test_ls.append(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                finalerror=test_err\n",
    "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                start = end\n",
    "\n",
    "        for users, items, rates in iter_final: #test의 pred_batch\n",
    "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
    "                                                    item_batch: items,                                                                                             \n",
    "                                                    phase:False})\n",
    "\n",
    "            p_ls.append(p_mat)\n",
    "            s_ls.append(s_mat)\n",
    "\n",
    "            concat_p = np.vstack(p_ls)\n",
    "            concat_s = np.vstack(s_ls)\n",
    "\n",
    "            user_arr = total_df.user.values\n",
    "            for idx, user in enumerate(user_arr) : \n",
    "                if user not in p_dict : \n",
    "                    p_dict[user] = concat_p[idx]\n",
    "\n",
    "            item_arr = total_df.image_id.values\n",
    "            for idx, item in enumerate(item_arr) : \n",
    "                if item not in s_dict : \n",
    "                    s_dict[item] = concat_s[idx]\n",
    "\n",
    "    pred = prediction_matrix(p_dict, s_dict)\n",
    "\n",
    "    # recommendation\n",
    "    users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
    "                                          columns = 'image_id', \n",
    "                                          values  = 'rate').fillna(0)\n",
    "\n",
    "\n",
    "    new_users_items_matrix_df  = pd.DataFrame(pred, \n",
    "                                              columns = users_items_matrix_df.columns, \n",
    "                                              index   = users_items_matrix_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
    "                                interact_matrix = new_users_items_matrix_df, \n",
    "                                df_content= itmDat)\n",
    "    \n",
    "    \n",
    "    r = new_users_items_matrix_df.values.astype(np.float64)\n",
    "    r[r == 0] = 'nan'\n",
    "    RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
    "  \n",
    "    \n",
    "    return RMSE, pred , recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 7801)\n",
      "(307, 7842)\n",
      "(7493, 7872)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 100)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 100)\n",
      "(?, 100)\n",
      "epoch train_error val_error elapsed_time\n",
      "  0,1.228821,1.196186,88.555722(s)\n",
      "  1,1.189911,1.196186,10.194758(s)\n",
      "  2,1.196383,1.196186,10.772735(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " array([[0.1817565 , 0.23254643, 0.31232224, ..., 0.06609091, 0.67512066,\n",
       "         0.29334075],\n",
       "        [0.23418616, 0.29056731, 0.37128781, ..., 0.07776848, 0.76635482,\n",
       "         0.34900335],\n",
       "        [0.20325486, 0.25424031, 0.32320394, ..., 0.07291699, 0.68020509,\n",
       "         0.3143198 ],\n",
       "        ...,\n",
       "        [0.21360675, 0.26584518, 0.31001409, ..., 0.06229653, 0.6740227 ,\n",
       "         0.3294215 ],\n",
       "        [0.1962347 , 0.24916302, 0.2935236 , ..., 0.07497258, 0.64163849,\n",
       "         0.28909368],\n",
       "        [0.22282136, 0.27888235, 0.35068094, ..., 0.07774318, 0.69713439,\n",
       "         0.34098785]]),\n",
       "              score                                    image_file_name  \\\n",
       " image_id                                                                \n",
       " 3599      0.933088  https://wearlyimages.s3.amazonaws.com/wearly/i...   \n",
       " 6853      0.911520  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 6813      0.903247  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 751       0.893710  https://wearlyimages.s3.amazonaws.com/wearly/f...   \n",
       " 5491      0.869055  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 1063      0.858386  https://wearlyimages.s3.amazonaws.com/wearly/f...   \n",
       " \n",
       "            hashtag_crawl  \n",
       " image_id                  \n",
       " 3599        instafashion  \n",
       " 6853             stylish  \n",
       " 6813             stylish  \n",
       " 751          fashionable  \n",
       " 5491         streetstyle  \n",
       " 1063      fashionblogger  )"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphRec(20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 7801)\n",
      "(307, 7842)\n",
      "(7493, 7872)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "inferenceDense() missing 1 required positional argument: 'MFSIZE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3efc57e1055f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGraphRec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-7ef0b08d13e8>\u001b[0m in \u001b[0;36mGraphRec\u001b[1;34m(Epoch)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0minfer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minferenceDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_user\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSER_NUM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mITEM_NUM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.09\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: inferenceDense() missing 1 required positional argument: 'MFSIZE'"
     ]
    }
   ],
   "source": [
    "GraphRec(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRec_NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_userANDrate():\n",
    "    credentials = \"postgresql://jczdtzmaouemml:f3f55cc0c6bd25a42866864c9299f4ec79ff4ff890f6f69467e2b14dc0010074@ec2-3-215-207-12.compute-1.amazonaws.com:5432/de8i6u9p9i6vq8\"\n",
    "    dbdf = pd.read_sql(\"\"\"select distinct * from wearly_user where age <= 70 order by idx \"\"\", con = credentials)\n",
    "    item = pd.read_sql(\"\"\"select * from wearly_wear\"\"\", con = credentials)\n",
    "    df = dbdf.copy()\n",
    "    df = df.drop_duplicates(['name', 'gender', 'age']+ df.columns.tolist()[4:-1], keep='first').reset_index(drop=False)\n",
    "    df = df.drop(['index','name','idx','time'], axis=1 , inplace=False)\n",
    "    \n",
    "    print('데이터부르기 완료')\n",
    "\n",
    "    v = [data_row.values.tolist()[2:] for index, data_row in df.iterrows()]\n",
    "    vv = [v[i][j] for i in range(len(v)) for j in range(len(v[i]))]\n",
    "\n",
    "    user_rt = pd.DataFrame(index=range(0,len(df)*100) , columns=['user','image_file_name', 'rate'])\n",
    "    user_rt['user'] = sorted([i for i in range(0,len(df)) for j in range(0,100)])\n",
    "    user_rt['image_file_name'] = vv\n",
    "\n",
    "    for i in range(len(user_rt)):\n",
    "      user_rt['rate'][i] = int(user_rt['image_file_name'][i][-1])\n",
    "      user_rt['image_file_name'][i] = str(user_rt['image_file_name'][i][:-1])\n",
    "    \n",
    "    user_rt = user_rt.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    user_rt = user_rt.merge(item[['image_id', 'image_file_name']], on='image_file_name')\n",
    "    print('user_rt 생성 완료')\n",
    "    \n",
    "    user_rt.loc[user_rt['rate'] == 3 , 'rate'] = 2\n",
    "    user_rt.loc[user_rt['rate'] == 5 , 'rate'] = 3 \n",
    "    print('숫자바꾸기 완료')\n",
    "    PERC = 0.9\n",
    "    rows = len(user_rt)\n",
    "    split_index = int(rows * PERC)\n",
    "    df_train = user_rt[0:split_index]\n",
    "    df_test = user_rt[split_index:].reset_index(drop=True)\n",
    "    print('split 완료')\n",
    "    user_mt = df[['age','gender']]\n",
    "    user_mt = user_mt.reset_index()\n",
    "    user_mt['user'] = user_mt['index']\n",
    "    user_mt = user_mt[['user', 'age', 'gender']]\n",
    "  \n",
    "    return user_mt, df_train, df_test , item, user_rt\n",
    "\n",
    "#UsrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
    "        out = self.inputs[ids, :] #뭐임?\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "def inferenceDense(MFSIZE ,phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1, UW=0.05, IW=0.02):\n",
    "    with tf.device('/cpu:0'): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
    "                \n",
    "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(ul1mf.shape)\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(il1mf.shape)\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
    "        print(InferInputMF.shape)\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
    "    return infer, regularizer, ul1mf, il1mf\n",
    "\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
    "    with tf.device('/cpu:0'):\n",
    "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
    "        assert global_step is not None\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
    "        cost = tf.add(cost_l2, regularizer)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op\n",
    "\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 3.0) #벗어나는 값들 위치시키기\n",
    "\n",
    "def prediction_matrix(user_dict, item_dict):\n",
    "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
    "  for i in range(len(user_dict)):\n",
    "    for j in range(len(item_dict)):\n",
    "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
    "  return pred\n",
    "\n",
    "\n",
    "def recommender_for_user(users_items_matrix_df ,user_id, interact_matrix, df_content, topn = 6):\n",
    "    '''\n",
    "    Recommender Games for UserWarning\n",
    "    '''\n",
    "    pred_scores = interact_matrix.loc[user_id].values\n",
    "\n",
    "    df_scores   = pd.DataFrame({'image_id': list(users_items_matrix_df.columns), \n",
    "                               'score': pred_scores})\n",
    "\n",
    "    df_rec      = df_scores.set_index('image_id')\\\n",
    "                    .join(df_content.set_index('image_id'))\\\n",
    "                    .sort_values('score', ascending=False)\\\n",
    "                    .head(topn)[['score', 'image_file_name', 'hashtag_crawl']]\n",
    "    \n",
    "    return df_rec[df_rec.score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec(Mf,Epoch, USERAGE, ITEMHASH, ITEMSTYLE, NORM):\n",
    "    tf.reset_default_graph()\n",
    "    usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
    "\n",
    "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
    "    BATCH_SIZE = 1000\n",
    "    DEVICE=\"/cpu:0\"\n",
    "    #With Graph Features\n",
    "    MFSIZE= Mf\n",
    "    UW=0.05\n",
    "    IW=0.02\n",
    "    LR=0.00003\n",
    "    EPOCH_MAX = Epoch\n",
    "\n",
    "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
    "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name'],axis=1, inplace=False)\n",
    "\n",
    "    if (USERAGE): \n",
    "      UsrDat = pd.get_dummies(UsrDat, columns=[ \"age\", \"gender\"])\n",
    "    else:\n",
    "      UsrDat = UsrDat.drop(['age'], axis=1 , inplace=False)\n",
    "      UsrDat = pd.get_dummies(UsrDat, columns=[\"gender\"])\n",
    "\n",
    "\n",
    "    if (ITEMHASH):\n",
    "      ItmDat = ItmDat\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(itmDat.columns[8:56].tolist() , axis=1 , inplace=False)\n",
    "\n",
    "\n",
    "    if (ITEMSTYLE):\n",
    "      ItmDat = ItmDat\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(itmDat.columns[56:].tolist() , axis=1 , inplace=False)\n",
    "\n",
    "    print(ItmDat.columns)\n",
    "    if (NORM):\n",
    "      ItmDat['like_num'] = np.log10(ItmDat['like_num'].values + 1)\n",
    "      ItmDat['comment_num'] = np.log10(ItmDat['comment_num'].values + 1)\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(['comment_num','like_num'],axis=1, inplace=False)\n",
    "\n",
    "\n",
    "    UsrDat = UsrDat.values\n",
    "    ItmDat = ItmDat.values\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "\n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in df_train.iterrows():\n",
    "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "        itemid=int(row['image_id'])\n",
    "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
    "        DegreeUsers[userid][0]+=1\n",
    "        DegreeItems[itemid][0]+=1\n",
    "\n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "\n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "\n",
    "\n",
    "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "    print(UserFeatures.shape) \n",
    "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "    UserFeaturesLength=UserFeatures.shape[1]\n",
    "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "    print(UserFeatures.shape)\n",
    "    print(ItemFeatures.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
    "\n",
    "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
    "\n",
    "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
    "\n",
    "    #tensor 값을 할당할 placeholder 생성\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "    #tensor matrix생성\n",
    "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
    "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
    "\n",
    "\n",
    "    infer, regularizer, p,s = inferenceDense(MFSIZE ,phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    finalerror=-1\n",
    "    #train_ls = []\n",
    "    #test_ls= []\n",
    "    p_ls = []\n",
    "    s_ls = []\n",
    "\n",
    "    p_dict = dict()\n",
    "    s_dict = dict()\n",
    "\n",
    "    total_df = user_rt.copy()\n",
    "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
    "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
    "                                                                  item_batch: items,\n",
    "                                                                  rate_batch: rates,\n",
    "                                                                  phase:True})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            #train_ls.append(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                degreelist=list()\n",
    "                predlist=list()\n",
    "                for users, items, rates in iter_test: #test의 pred_batch\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items,                                                                                             \n",
    "                                                            phase:False})\n",
    "\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    #test_ls.append(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                finalerror=test_err\n",
    "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                start = end\n",
    "\n",
    "        for users, items, rates in iter_final: #test의 pred_batch\n",
    "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
    "                                                    item_batch: items,                                                                                             \n",
    "                                                    phase:False})\n",
    "\n",
    "            p_ls.append(p_mat)\n",
    "            s_ls.append(s_mat)\n",
    "\n",
    "            concat_p = np.vstack(p_ls)\n",
    "            concat_s = np.vstack(s_ls)\n",
    "\n",
    "            user_arr = total_df.user.values\n",
    "            for idx, user in enumerate(user_arr) : \n",
    "                if user not in p_dict : \n",
    "                    p_dict[user] = concat_p[idx]\n",
    "\n",
    "            item_arr = total_df.image_id.values\n",
    "            for idx, item in enumerate(item_arr) : \n",
    "                if item not in s_dict : \n",
    "                    s_dict[item] = concat_s[idx]\n",
    "\n",
    "    pred = prediction_matrix(p_dict, s_dict)\n",
    "\n",
    "    # recommendation\n",
    "    users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
    "                                          columns = 'image_id', \n",
    "                                          values  = 'rate').fillna(0)\n",
    "\n",
    "\n",
    "    new_users_items_matrix_df  = pd.DataFrame(pred, \n",
    "                                              columns = users_items_matrix_df.columns, \n",
    "                                              index   = users_items_matrix_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
    "                                interact_matrix = new_users_items_matrix_df, \n",
    "                                df_content= itmDat)\n",
    "    \n",
    "    \n",
    "    r = users_items_matrix_df.values.astype(np.float64)\n",
    "    r[r == 0] = 'nan'\n",
    "    RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
    "    print('Final Rmse :' , RMSE)\n",
    "    \n",
    "    return pred,recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터부르기 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_rt 생성 완료\n",
      "숫자바꾸기 완료\n",
      "split 완료\n",
      "Index(['like_num', 'comment_num', 'fashion', 'ootd', 'fashionblogger',\n",
      "       'instafashion', 'fashionista', 'streetstyle', 'outfit', 'instagood',\n",
      "       'fashionable', 'fashionstyle', 'stylish', 'outfitoftheday',\n",
      "       'styleblogger', 'moda', 'love', 'model', 'look', 'streetwear',\n",
      "       'photooftheday', 'streetfashion', 'instastyle', 'fashionweek',\n",
      "       'photography', 'trend', 'fashiongram', 'beautiful', 'fashionblog',\n",
      "       'fashionaddict', 'beauty', 'summer', 'fashiondiaries', 'fashionpost',\n",
      "       'fashioninspiration', 'lookoftheday', 'dress', 'blogger', 'picoftheday',\n",
      "       'lookbook', 'girl', 'mensfashion', 'cute', 'follow', 'instagram',\n",
      "       'fashionshow', 'lifestyle', 'shopping', 'fashioninsta', 'dailylook',\n",
      "       'sporty', 'casual', 'modern', 'elegant', 'natural', 'glamorous',\n",
      "       'sophisticated', 'grunge', 'retro', 'romantic', 'sexy', 'military',\n",
      "       'ethnic', 'classic', 'business_casual', 'manish', 'exotic',\n",
      "       'goth_punk_rocker', 'hiphop', 'hippie', 'tomboy', 'preppy',\n",
      "       'kitsch_kidult'],\n",
      "      dtype='object')\n",
      "(307, 7801)\n",
      "(307, 7842)\n",
      "(7493, 7874)\n",
      "WARNING:tensorflow:From <ipython-input-26-2f9887780465>:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020605D9EB88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020605D9EB88>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020605D9EB88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020605D9EB88>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 80)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020604CCE708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020604CCE708>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020604CCE708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020604CCE708>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 80)\n",
      "(?, 80)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-29-bfe54d177858>:104: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "epoch train_error val_error elapsed_time\n",
      "  0,1.195408,1.196186,56.229592(s)\n",
      "  1,1.194354,1.196186,5.993980(s)\n",
      "Final Rmse : 1.9715167382531724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.12449763, 0.13904666, 0.18757683, ..., 0.03613424, 0.24227862,\n",
       "         0.06987939],\n",
       "        [0.10000271, 0.14835826, 0.1983696 , ..., 0.05213572, 0.26549237,\n",
       "         0.12146311],\n",
       "        [0.09060855, 0.11641113, 0.17084675, ..., 0.05033687, 0.23054113,\n",
       "         0.12273507],\n",
       "        ...,\n",
       "        [0.12603373, 0.14432936, 0.21268453, ..., 0.06179281, 0.26275286,\n",
       "         0.1484893 ],\n",
       "        [0.10919257, 0.13407295, 0.21166253, ..., 0.05976913, 0.21526776,\n",
       "         0.08674096],\n",
       "        [0.13994101, 0.16456171, 0.21200122, ..., 0.04921375, 0.26780201,\n",
       "         0.11682714]]),\n",
       "              score                                    image_file_name  \\\n",
       " image_id                                                                \n",
       " 6813      0.395460  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 2661      0.375878  https://wearlyimages.s3.amazonaws.com/wearly/f...   \n",
       " 6853      0.374970  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 4022      0.370979  https://wearlyimages.s3.amazonaws.com/wearly/l...   \n",
       " 5491      0.362988  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 3599      0.358666  https://wearlyimages.s3.amazonaws.com/wearly/i...   \n",
       " \n",
       "          hashtag_crawl  \n",
       " image_id                \n",
       " 6813           stylish  \n",
       " 2661      fashionstyle  \n",
       " 6853           stylish  \n",
       " 4022              look  \n",
       " 5491       streetstyle  \n",
       " 3599      instafashion  )"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphRec(40,2,USERAGE=True,ITEMHASH=True,ITEMSTYLE= True,NORM = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec_record(Mf,Epoch, USERAGE=True, ITEMHASH=True, ITEMSTYLE=True, NORM=True):\n",
    "    tf.reset_default_graph()\n",
    "    usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
    "\n",
    "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
    "    BATCH_SIZE = 1000\n",
    "    DEVICE=\"/cpu:0\"\n",
    "    #With Graph Features\n",
    "    MFSIZE= Mf\n",
    "    UW=0.05\n",
    "    IW=0.02\n",
    "    LR=0.00003\n",
    "    EPOCH_MAX = Epoch\n",
    "\n",
    "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
    "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name'],axis=1, inplace=False)\n",
    "\n",
    "    if (USERAGE): \n",
    "      UsrDat = pd.get_dummies(UsrDat, columns=[ \"age\", \"gender\"])\n",
    "    else:\n",
    "      UsrDat = UsrDat.drop(['age'], axis=1 , inplace=False)\n",
    "      UsrDat = pd.get_dummies(UsrDat, columns=[\"gender\"])\n",
    "\n",
    "\n",
    "    if (ITEMHASH):\n",
    "      ItmDat = ItmDat\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(itmDat.columns[8:56].tolist() , axis=1 , inplace=False)\n",
    "\n",
    "\n",
    "    if (ITEMSTYLE):\n",
    "      ItmDat = ItmDat\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(itmDat.columns[56:].tolist() , axis=1 , inplace=False)\n",
    "\n",
    "    print(ItmDat.columns)\n",
    "    if (NORM):\n",
    "      ItmDat['like_num'] = np.log10(ItmDat['like_num'].values + 1)\n",
    "      ItmDat['comment_num'] = np.log10(ItmDat['comment_num'].values + 1)\n",
    "    else:\n",
    "      ItmDat = ItmDat.drop(['comment_num','like_num'],axis=1, inplace=False)\n",
    "\n",
    "\n",
    "    UsrDat = UsrDat.values\n",
    "    ItmDat = ItmDat.values\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "\n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in df_train.iterrows():\n",
    "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "        itemid=int(row['image_id'])\n",
    "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
    "        DegreeUsers[userid][0]+=1\n",
    "        DegreeItems[itemid][0]+=1\n",
    "\n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "\n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "\n",
    "\n",
    "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "    print(UserFeatures.shape) \n",
    "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "    UserFeaturesLength=UserFeatures.shape[1]\n",
    "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "    print(UserFeatures.shape)\n",
    "    print(ItemFeatures.shape)\n",
    "    \n",
    "    save_file = './wear_record/train_model.ckpt'\n",
    "    \n",
    "    # saver 객체 생성\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "\n",
    "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
    "\n",
    "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
    "\n",
    "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
    "\n",
    "    #tensor 값을 할당할 placeholder 생성\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "    #tensor matrix생성\n",
    "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
    "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
    "\n",
    "\n",
    "    infer, regularizer, p,s = inferenceDense(MFSIZE ,phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    finalerror=-1\n",
    "    #train_ls = []\n",
    "    #test_ls= []\n",
    "    p_ls = []\n",
    "    s_ls = []\n",
    "\n",
    "    p_dict = dict()\n",
    "    s_dict = dict()\n",
    "\n",
    "    total_df = user_rt.copy()\n",
    "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
    "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
    "                                                                  item_batch: items,\n",
    "                                                                  rate_batch: rates,\n",
    "                                                                  phase:True})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            #train_ls.append(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                degreelist=list()\n",
    "                predlist=list()\n",
    "                for users, items, rates in iter_test: #test의 pred_batch\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items,                                                                                             \n",
    "                                                            phase:False})\n",
    "\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    #test_ls.append(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                finalerror=test_err\n",
    "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                start = end\n",
    "                \n",
    "        a = tf.Variable(pred_batch)\n",
    "        b = tf.Variable(p_mat)\n",
    "        c = tf.Variable(s_mat)\n",
    "                \n",
    "        saver.save(sess, save_file)\n",
    "        print('Trained_model_saved')\n",
    "\n",
    "#         for users, items, rates in iter_final: #test의 pred_batch\n",
    "#             pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
    "#                                                     item_batch: items,                                                                                             \n",
    "#                                                     phase:False})\n",
    "\n",
    "#             p_ls.append(p_mat)\n",
    "#             s_ls.append(s_mat)\n",
    "\n",
    "#             concat_p = np.vstack(p_ls)\n",
    "#             concat_s = np.vstack(s_ls)\n",
    "\n",
    "#             user_arr = total_df.user.values\n",
    "#             for idx, user in enumerate(user_arr) : \n",
    "#                 if user not in p_dict : \n",
    "#                     p_dict[user] = concat_p[idx]\n",
    "\n",
    "#             item_arr = total_df.image_id.values\n",
    "#             for idx, item in enumerate(item_arr) : \n",
    "#                 if item not in s_dict : \n",
    "#                     s_dict[item] = concat_s[idx]\n",
    "\n",
    "#     pred = prediction_matrix(p_dict, s_dict)\n",
    "\n",
    "#     # recommendation\n",
    "#     users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
    "#                                           columns = 'image_id', \n",
    "#                                           values  = 'rate').fillna(0)\n",
    "\n",
    "\n",
    "#     new_users_items_matrix_df  = pd.DataFrame(pred, \n",
    "#                                               columns = users_items_matrix_df.columns, \n",
    "#                                               index   = users_items_matrix_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
    "#                                 interact_matrix = new_users_items_matrix_df, \n",
    "#                                 df_content= itmDat)\n",
    "    \n",
    "    \n",
    "#     r = users_items_matrix_df.values.astype(np.float64)\n",
    "#     r[r == 0] = 'nan'\n",
    "#     RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
    "#     print('Final Rmse :' , RMSE)\n",
    "    \n",
    "#     return pred,recom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터부르기 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_rt 생성 완료\n",
      "숫자바꾸기 완료\n",
      "split 완료\n",
      "Index(['like_num', 'comment_num', 'fashion', 'ootd', 'fashionblogger',\n",
      "       'instafashion', 'fashionista', 'streetstyle', 'outfit', 'instagood',\n",
      "       'fashionable', 'fashionstyle', 'stylish', 'outfitoftheday',\n",
      "       'styleblogger', 'moda', 'love', 'model', 'look', 'streetwear',\n",
      "       'photooftheday', 'streetfashion', 'instastyle', 'fashionweek',\n",
      "       'photography', 'trend', 'fashiongram', 'beautiful', 'fashionblog',\n",
      "       'fashionaddict', 'beauty', 'summer', 'fashiondiaries', 'fashionpost',\n",
      "       'fashioninspiration', 'lookoftheday', 'dress', 'blogger', 'picoftheday',\n",
      "       'lookbook', 'girl', 'mensfashion', 'cute', 'follow', 'instagram',\n",
      "       'fashionshow', 'lifestyle', 'shopping', 'fashioninsta', 'dailylook',\n",
      "       'sporty', 'casual', 'modern', 'elegant', 'natural', 'glamorous',\n",
      "       'sophisticated', 'grunge', 'retro', 'romantic', 'sexy', 'military',\n",
      "       'ethnic', 'classic', 'business_casual', 'manish', 'exotic',\n",
      "       'goth_punk_rocker', 'hiphop', 'hippie', 'tomboy', 'preppy',\n",
      "       'kitsch_kidult'],\n",
      "      dtype='object')\n",
      "(307, 7801)\n",
      "(307, 7842)\n",
      "(7493, 7874)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-0e1bda5fa188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGraphRec_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUSERAGE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mITEMHASH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mITEMSTYLE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-f993e912bf3f>\u001b[0m in \u001b[0;36mGraphRec_record\u001b[1;34m(Mf, Epoch, USERAGE, ITEMHASH, ITEMSTYLE, NORM)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# saver 객체 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m    823\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[0;32m    824\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m    860\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "GraphRec_record(10,2, USERAGE=True, ITEMHASH=True, ITEMSTYLE=True, NORM=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./wear_record/test_model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'user_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-fa0b908d4d61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./wear_record/test_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# graph = tf.get_default_graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# 불러오기\n",
    "iter_final = OneEpochIterator([user_rt[\"user\"],user_rt[\"image_id\"],user_rt[\"rate\"]],batch_size=len(user_rt)) #10000개?\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph('./wear_record/test_model.meta')\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, './wear_record/test_model')\n",
    "print(user_batch)\n",
    "\n",
    "# graph = tf.get_default_graph()\n",
    "# w1 = graph.get_tensor_by_name(\"id_user\")\n",
    "# w2 = graph.get_tensor_by_name(\"id_item\")\n",
    "# feed_dict ={w1:13.0,w2:17.0}\n",
    "\n",
    "\n",
    "# for users, items, rates in iter_final: #test의 pred_batch\n",
    "#     pred_batch, p_mat, s_mat = sess.run([infer,p,s], feed_dict={user_batch: \"id_user\",\n",
    "#                                             item_batch: \"id_item\",                                                                                             \n",
    "#                                             phase:False})\n",
    "\n",
    "#     p_ls.append(p_mat)\n",
    "#     s_ls.append(s_mat)\n",
    "\n",
    "#     concat_p = np.vstack(p_ls)\n",
    "#     concat_s = np.vstack(s_ls)\n",
    "\n",
    "#     user_arr = user_rt.user.values\n",
    "#     for idx, user in enumerate(user_arr) : \n",
    "#         if user not in p_dict : \n",
    "#             p_dict[user] = concat_p[idx]\n",
    "\n",
    "#     item_arr = user_rt.image_id.values\n",
    "#     for idx, item in enumerate(item_arr) : \n",
    "#         if item not in s_dict : \n",
    "#             s_dict[item] = concat_s[idx]\n",
    "\n",
    "# pred = prediction_matrix(p_dict, s_dict)\n",
    "\n",
    "# # recommendation\n",
    "# users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
    "#                                   columns = 'image_id', \n",
    "#                                   values  = 'rate').fillna(0)\n",
    "\n",
    "\n",
    "# new_users_items_matrix_df  = pd.DataFrame(pred, \n",
    "#                                       columns = users_items_matrix_df.columns, \n",
    "#                                       index   = users_items_matrix_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
    "#                         interact_matrix = new_users_items_matrix_df, \n",
    "#                         df_content= itmDat)\n",
    "\n",
    "\n",
    "# r = users_items_matrix_df.values.astype(np.float64)\n",
    "# r[r == 0] = 'nan'\n",
    "# RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
    "# print('Final Rmse :' , RMSE)\n",
    "\n",
    "# print(recom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
