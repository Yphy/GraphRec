{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터부르기 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\yphy0\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_rt 생성 완료\n",
      "숫자바꾸기 완료\n",
      "split 완료\n"
     ]
    }
   ],
   "source": [
    "def make_userANDrate():\n",
    "    credentials = \"postgresql://jczdtzmaouemml:f3f55cc0c6bd25a42866864c9299f4ec79ff4ff890f6f69467e2b14dc0010074@ec2-3-215-207-12.compute-1.amazonaws.com:5432/de8i6u9p9i6vq8\"\n",
    "    dbdf = pd.read_sql(\"\"\"select distinct * from wearly_user where age <= 70 order by idx \"\"\", con = credentials)\n",
    "    item = pd.read_sql(\"\"\"select * from wearly_wear\"\"\", con = credentials)\n",
    "    df = dbdf.copy()\n",
    "    df = df.drop_duplicates(['name', 'gender', 'age']+ df.columns.tolist()[4:-1], keep='first').reset_index(drop=False)\n",
    "    df = df.drop(['index','name','idx','time'], axis=1 , inplace=False)\n",
    "    \n",
    "    print('데이터부르기 완료')\n",
    "\n",
    "    v = [data_row.values.tolist()[2:] for index, data_row in df.iterrows()]\n",
    "    vv = [v[i][j] for i in range(len(v)) for j in range(len(v[i]))]\n",
    "\n",
    "    user_rt = pd.DataFrame(index=range(0,len(df)*100) , columns=['user','image_file_name', 'rate'])\n",
    "    user_rt['user'] = sorted([i for i in range(0,len(df)) for j in range(0,100)])\n",
    "    user_rt['image_file_name'] = vv\n",
    "\n",
    "    for i in range(len(user_rt)):\n",
    "      user_rt['rate'][i] = int(user_rt['image_file_name'][i][-1])\n",
    "      user_rt['image_file_name'][i] = str(user_rt['image_file_name'][i][:-1])\n",
    "    \n",
    "    user_rt = user_rt.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    user_rt = user_rt.merge(item[['image_id', 'image_file_name']], on='image_file_name')\n",
    "    print('user_rt 생성 완료')\n",
    "    \n",
    "    user_rt.loc[user_rt['rate'] == 3 , 'rate'] = 2\n",
    "    user_rt.loc[user_rt['rate'] == 5 , 'rate'] = 3 \n",
    "    print('숫자바꾸기 완료')\n",
    "    PERC = 0.9\n",
    "    rows = len(user_rt)\n",
    "    split_index = int(rows * PERC)\n",
    "    df_train = user_rt[0:split_index]\n",
    "    df_test = user_rt[split_index:].reset_index(drop=True)\n",
    "    print('split 완료')\n",
    "    user_mt = df[['age','gender']]\n",
    "    user_mt = user_mt.reset_index()\n",
    "    user_mt['user'] = user_mt['index']\n",
    "    user_mt = user_mt[['user', 'age', 'gender']]\n",
    "    \n",
    "    # get one-hot encoding (gender)\n",
    "    user_mt = pd.get_dummies(user_mt, columns=[ \"age\", \"gender\"])\n",
    "  \n",
    "    return user_mt, df_train, df_test , item, user_rt\n",
    "\n",
    "usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,)) #0과 len사이의  batch_size 크기의 랜덤 정수 생성\n",
    "        out = self.inputs[ids, :] #뭐임?\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochIterator(ShuffleIterator):\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size)) #len 만큼의 array를 len/batch size의 올림만큼 분할\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "def inferenceDense(phase,user_batch, item_batch,idx_user,idx_item, user_num, item_num,UReg=0.05,IReg=0.1, UW=0.05, IW=0.02):\n",
    "    with tf.device('/cpu:0'): \n",
    "        user_batch = tf.nn.embedding_lookup(idx_user, user_batch, name=\"embedding_user\") #idx_user에서 user_batch의 index값을 뽑음\n",
    "        item_batch = tf.nn.embedding_lookup(idx_item, item_batch, name=\"embedding_item\") #w_item 이 들어감\n",
    "                \n",
    "        ul1mf=tf.layers.dense(inputs=user_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(ul1mf.shape)\n",
    "        il1mf=tf.layers.dense(inputs=item_batch, units=MFSIZE,activation=tf.nn.crelu, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        print(il1mf.shape)\n",
    "        InferInputMF=tf.multiply(ul1mf, il1mf)\n",
    "        print(InferInputMF.shape)\n",
    "\n",
    "        infer=tf.reduce_sum(InferInputMF, 1, name=\"inference\") #reduce_sum은 모든 차원제거하고 원소합\n",
    "\n",
    "        regularizer = tf.add(UW*tf.nn.l2_loss(ul1mf), IW*tf.nn.l2_loss(il1mf), name=\"regularizer\") # l2 regularize\n",
    "    return infer, regularizer, ul1mf, il1mf\n",
    "\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.0005, reg=0.1):\n",
    "    with tf.device('/cpu:0'):\n",
    "        global_step = tf.train.get_global_step() #훈련 중단시 체크포인트\n",
    "        assert global_step is not None\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch)) #infer - rate_batch?\n",
    "        cost = tf.add(cost_l2, regularizer)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op\n",
    "\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 3.0) #벗어나는 값들 위치시키기\n",
    "\n",
    "def prediction_matrix(user_dict, item_dict):\n",
    "  pred = np.zeros((len(user_dict), len(item_dict)))\n",
    "  for i in range(len(user_dict)):\n",
    "    for j in range(len(item_dict)):\n",
    "      pred[i][j] += np.dot(user_dict[i],item_dict[j])\n",
    "  return pred\n",
    "\n",
    "\n",
    "def recommender_for_user(users_items_matrix_df ,user_id, interact_matrix, df_content, topn = 6):\n",
    "    '''\n",
    "    Recommender Games for UserWarning\n",
    "    '''\n",
    "    pred_scores = interact_matrix.loc[user_id].values\n",
    "\n",
    "    df_scores   = pd.DataFrame({'image_id': list(users_items_matrix_df.columns), \n",
    "                               'score': pred_scores})\n",
    "\n",
    "    df_rec      = df_scores.set_index('image_id')\\\n",
    "                    .join(df_content.set_index('image_id'))\\\n",
    "                    .sort_values('score', ascending=False)\\\n",
    "                    .head(topn)[['score', 'image_file_name', 'hashtag_crawl']]\n",
    "    \n",
    "    return df_rec[df_rec.score > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RMSE(df1,df2):\n",
    "#     raw = df1\n",
    "    raw2 = df1.values.astype(np.float64)\n",
    "    pred = df2\n",
    "    \n",
    "    raw2[raw2 ==0] = 'nan'\n",
    "    \n",
    "    return np.sqrt(np.nansum((raw2 - pred)**2 / np.isfinite(raw2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9598431632269921"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_RMSE(users_items_matrix_df,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphRec(Mf,Epoch):\n",
    "    tf.reset_default_graph()\n",
    "    # usrDat, df_train, df_test, itmDat, user_rt = make_userANDrate()\n",
    "\n",
    "    USER_NUM =len(usrDat)  ; ITEM_NUM = user_rt.image_id.nunique()\n",
    "    BATCH_SIZE = 1000\n",
    "    DEVICE=\"/cpu:0\"\n",
    "    #With Graph Features\n",
    "    MFSIZE= Mf\n",
    "    UW=0.05\n",
    "    IW=0.02\n",
    "    LR=0.00003\n",
    "    EPOCH_MAX = Epoch\n",
    "\n",
    "    UsrDat = usrDat.drop(['user'],axis=1, inplace=False)\n",
    "    ItmDat = itmDat.drop(['idx', 'image_id','post_id', 'image_file_name' , 'hashtag_crawl', 'account_name','comment_num','like_num'],axis=1, inplace=False)\n",
    "\n",
    "    UsrDat = UsrDat.values\n",
    "    ItmDat = ItmDat.values\n",
    "\n",
    "    AdjacencyUsers = np.zeros((USER_NUM,ITEM_NUM), dtype=np.float32) # N x M shape의 zero matrix 생성 (Adjacency)\n",
    "    DegreeUsers = np.zeros((USER_NUM,1), dtype=np.float32) #N x 1  shape의 zero vactor 생성 (Degree)\n",
    "\n",
    "    AdjacencyItems = np.zeros((ITEM_NUM,USER_NUM), dtype=np.float32) # M x N shape의 zero matrix 생성\n",
    "    DegreeItems =  np.zeros((ITEM_NUM,1), dtype=np.float32)  # M X 1 shape의 zero vactor 생성\n",
    "    for index, row in df_train.iterrows():\n",
    "        userid=int(row['user']) #row돌면서 'user'와 'item' column의 값 저장\n",
    "        itemid=int(row['image_id'])\n",
    "        AdjacencyUsers[userid][itemid]=row['rate']/3.0 #train set의 rating / max 값을 numpy matrix에 저장\n",
    "        AdjacencyItems[itemid][userid]=row['rate']/3.0 #동일, transpose matrix에\n",
    "        DegreeUsers[userid][0]+=1\n",
    "        DegreeItems[itemid][0]+=1\n",
    "\n",
    "    DUserMax=np.amax(DegreeUsers) #max값\n",
    "    DItemMax=np.amax(DegreeItems)\n",
    "    DegreeUsers=np.true_divide(DegreeUsers, DUserMax) #DegreeUsers의 array들 전부를 Max값으로 나누기\n",
    "    DegreeItems=np.true_divide(DegreeItems, DItemMax)\n",
    "\n",
    "    AdjacencyUsers=np.asarray(AdjacencyUsers,dtype=np.float32) #정규화된 rating이 적힌 matrix를 array로\n",
    "    AdjacencyItems=np.asarray(AdjacencyItems,dtype=np.float32)\n",
    "\n",
    "\n",
    "    UserFeatures= np.concatenate((np.identity(USER_NUM,dtype=np.bool_), AdjacencyUsers,DegreeUsers), axis=1)#np.identity concat\n",
    "    print(UserFeatures.shape) #\n",
    "    ItemFeatures= np.concatenate((np.identity(ITEM_NUM,dtype=np.bool_), AdjacencyItems,DegreeItems), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "    UserFeatures=np.concatenate((UserFeatures,UsrDat), axis=1) \n",
    "\n",
    "    ItemFeatures=np.concatenate((ItemFeatures,ItmDat), axis=1) \n",
    "\n",
    "    UserFeaturesLength=UserFeatures.shape[1]\n",
    "    ItemFeaturesLength=ItemFeatures.shape[1]\n",
    "\n",
    "    print(UserFeatures.shape)\n",
    "    print(ItemFeatures.shape)\n",
    "\n",
    "\n",
    "    samples_per_batch = len(df_train) // BATCH_SIZE #90000 / 1000 = 90\n",
    "\n",
    "    iter_train = ShuffleIterator([df_train[\"user\"],df_train[\"image_id\"],df_train[\"rate\"]],batch_size=BATCH_SIZE) #1000 X 90 이나옴\n",
    "\n",
    "    iter_test = OneEpochIterator([df_test[\"user\"],df_test[\"image_id\"],df_test[\"rate\"]],batch_size=10000) #10000개?\n",
    "\n",
    "    #tensor 값을 할당할 placeholder 생성\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\") #dtype,shape default\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float64, shape=[None])\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "    #tensor matrix생성\n",
    "    w_user = tf.constant(UserFeatures,name=\"userids\", shape=[USER_NUM,UserFeatures.shape[1]],dtype=tf.float64) #943x2710을 constant\n",
    "    w_item = tf.constant(ItemFeatures,name=\"itemids\", shape=[ITEM_NUM, ItemFeatures.shape[1]],dtype=tf.float64)#1682x2646\n",
    "\n",
    "\n",
    "    infer, regularizer, p,s = inferenceDense(phase,user_batch, item_batch,w_user,w_item, user_num=USER_NUM, item_num=ITEM_NUM)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=LR, reg=0.09)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    finalerror=-1\n",
    "    #train_ls = []\n",
    "    #test_ls= []\n",
    "    p_ls = []\n",
    "    s_ls = []\n",
    "\n",
    "    p_dict = dict()\n",
    "    s_dict = dict()\n",
    "\n",
    "    total_df = user_rt.copy()\n",
    "    iter_final = OneEpochIterator([total_df[\"user\"],total_df[\"image_id\"],total_df[\"rate\"]],batch_size=len(total_df)) #10000개?\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch): #10 X 90\n",
    "            #users, items, rates,y,m,d,dw,dy,w = next(iter_train)\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch,p_mat,s_mat  = sess.run([train_op, infer, p,s], feed_dict={user_batch: users,\n",
    "                                                                  item_batch: items,\n",
    "                                                                  rate_batch: rates,\n",
    "                                                                  phase:True})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            #train_ls.append(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0: #1 Epoch 일때마다 / batch가 90단위마다\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                degreelist=list()\n",
    "                predlist=list()\n",
    "                for users, items, rates in iter_test: #test의 pred_batch\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items,                                                                                             \n",
    "                                                            phase:False})\n",
    "\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    #test_ls.append(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                finalerror=test_err\n",
    "                print(\"{:3d},{:f},{:f},{:f}(s)\".format(i // samples_per_batch, train_err, test_err, end - start))\n",
    "                start = end\n",
    "\n",
    "        for users, items, rates in iter_final: #test의 pred_batch\n",
    "            pred_batch, p_mat, s_mat = sess.run([infer, p, s], feed_dict={user_batch: users,\n",
    "                                                    item_batch: items,                                                                                             \n",
    "                                                    phase:False})\n",
    "\n",
    "            p_ls.append(p_mat)\n",
    "            s_ls.append(s_mat)\n",
    "\n",
    "            concat_p = np.vstack(p_ls)\n",
    "            concat_s = np.vstack(s_ls)\n",
    "\n",
    "            user_arr = total_df.user.values\n",
    "            for idx, user in enumerate(user_arr) : \n",
    "                if user not in p_dict : \n",
    "                    p_dict[user] = concat_p[idx]\n",
    "\n",
    "            item_arr = total_df.image_id.values\n",
    "            for idx, item in enumerate(item_arr) : \n",
    "                if item not in s_dict : \n",
    "                    s_dict[item] = concat_s[idx]\n",
    "\n",
    "    pred = prediction_matrix(p_dict, s_dict)\n",
    "\n",
    "    # recommendation\n",
    "    users_items_matrix_df = user_rt.pivot(index   = 'user', \n",
    "                                          columns = 'image_id', \n",
    "                                          values  = 'rate').fillna(0)\n",
    "\n",
    "\n",
    "    new_users_items_matrix_df  = pd.DataFrame(pred, \n",
    "                                              columns = users_items_matrix_df.columns, \n",
    "                                              index   = users_items_matrix_df.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    recom = recommender_for_user(users_items_matrix_df ,user_id = usrDat['user'].values.tolist()[-1], \n",
    "                                interact_matrix = new_users_items_matrix_df, \n",
    "                                df_content= itmDat)\n",
    "    \n",
    "    \n",
    "    r = new_users_items_matrix_df.values.astype(np.float64)\n",
    "    r[r == 0] = 'nan'\n",
    "    RMSE = np.sqrt(np.nansum((r - pred)**2 / np.isfinite(r).sum()))\n",
    "  \n",
    "    \n",
    "    return RMSE, pred , recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 7801)\n",
      "(307, 7842)\n",
      "(7493, 7872)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569D079BC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 100)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002569FF55E48>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "(?, 100)\n",
      "(?, 100)\n",
      "epoch train_error val_error elapsed_time\n",
      "  0,1.228821,1.196186,88.555722(s)\n",
      "  1,1.189911,1.196186,10.194758(s)\n",
      "  2,1.196383,1.196186,10.772735(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " array([[0.1817565 , 0.23254643, 0.31232224, ..., 0.06609091, 0.67512066,\n",
       "         0.29334075],\n",
       "        [0.23418616, 0.29056731, 0.37128781, ..., 0.07776848, 0.76635482,\n",
       "         0.34900335],\n",
       "        [0.20325486, 0.25424031, 0.32320394, ..., 0.07291699, 0.68020509,\n",
       "         0.3143198 ],\n",
       "        ...,\n",
       "        [0.21360675, 0.26584518, 0.31001409, ..., 0.06229653, 0.6740227 ,\n",
       "         0.3294215 ],\n",
       "        [0.1962347 , 0.24916302, 0.2935236 , ..., 0.07497258, 0.64163849,\n",
       "         0.28909368],\n",
       "        [0.22282136, 0.27888235, 0.35068094, ..., 0.07774318, 0.69713439,\n",
       "         0.34098785]]),\n",
       "              score                                    image_file_name  \\\n",
       " image_id                                                                \n",
       " 3599      0.933088  https://wearlyimages.s3.amazonaws.com/wearly/i...   \n",
       " 6853      0.911520  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 6813      0.903247  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 751       0.893710  https://wearlyimages.s3.amazonaws.com/wearly/f...   \n",
       " 5491      0.869055  https://wearlyimages.s3.amazonaws.com/wearly/s...   \n",
       " 1063      0.858386  https://wearlyimages.s3.amazonaws.com/wearly/f...   \n",
       " \n",
       "            hashtag_crawl  \n",
       " image_id                  \n",
       " 3599        instafashion  \n",
       " 6853             stylish  \n",
       " 6813             stylish  \n",
       " 751          fashionable  \n",
       " 5491         streetstyle  \n",
       " 1063      fashionblogger  )"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphRec(20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
